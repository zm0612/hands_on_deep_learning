> 参考资料：https://zh-v2.d2l.ai/chapter_linear-networks/index.html

# 线性神经网络总结

## 1.线性回归

这一小结的内容比较简单，总结起来说就是一个线性拟合的过程。

需要关注的点有时候样本数据过大时，可以随机选取小批量样本进行随机梯度下降，从而拟合模型参数。避免了全数据的传递和求导等耗时操作。

另一个很重要的点是，对于数据尽量减少使用for循环的操作，能使用矩阵运算就最好选择矩阵运算，速度会得到巨大的提高。

**正态分布与平方损失**

线性回归最终转化为最小二乘求解的原理需要关注一下，这里需要应用条件概率来解释。

对于线性回归问题，我们已知模型的结构，但是不知道模型的参数，另外还知道样本的输入，以及样本输入对应的真实输出。

那么如果用概率来建模这个问题，那么就是在已知输入$x$的情况下，要使得测量值$w^Tx+b$尽量的接近真实值$y$，那么此时的置信度将达到最大，此时对应的$w,b$就是我们要求的最优估计。

因此采用数学公式表示如下：
$$
P(y|x) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp{(-\frac{1}{2\sigma^2}(y-w^Tx-b)^2)} \tag{1}
$$
上式表示，在x的条件下得到y的概率。更通俗的解释就是，当$x$取某一个值的时候，$w^T+b$会得到一个测量值，也就是当前条件下的y的期望值，由于期望值和实际的真实值之间有误差，那么就会导致(1)式不会取得最大值，但是如果通过优化$w,b$就等使得$y-w^T-b$接近0，那么此时(1)式子将会取得最大概率。

那么当我们有很多观测时，那么要做的就是使得整体的条件概率达到最大。写出最大似然估计如下：
$$
P(y|X) = \prod_{i=1}^n p(y^{i}|x^{i}) \tag{2}
$$
通过取负log最终得到下式：
$$
-logP(y|X) = \sum_{i=1}^n \frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}(y^{(i)}-w^Tx^{(i)}-b)^2 \tag{3}
$$
现在变成了最小化(3)式了，那么也就是所有误差取得最小的时候得到最大似然估计。

## 2. softmax回归

**softmax函数**

softmax是用来将输出结果转化为概率结果的函数，它保证了所有概率之和为1。其函数方程如下：
$$
\hat y = softmax(o), 其中 \hat y_j = \frac{\exp(o_j)}{\sum_k\exp(o_k)} \tag{4}
$$
**损失函数**

衡量softmax的输出值与实际值之间的误差，可以使用对数似然。

如果有n个样本以及样本的真实值，那么估计值与实际值可以进行比较：
$$
P(Y|X) = \prod_{i=1}^n p(y^{(i)}|x^{(i)}) \tag{5}
$$
根据最大似然估计的方法，对上式去负对数，那么就变成了最小化下式：
$$
-\log P(Y|X) = \sum_{i=1}^n-\log P(y^{(i)}|x^{(i)}) = \sum_{i=1}^nl(y^{(i)}, \hat y^{(i)}) \tag{6}
$$
对于softmax输出的$\hat y$，以及真实标签之间的误差为：
$$
l(y,\hat y) = - \sum_{j=1}^qy_j \log \hat y_j \tag{7}
$$

> $y_i$只有在真实值时为1， 其余均为。所以（7）式的求和符号可以省略。

因此，当将softmax函数带入（7）式：
$$
l(y,\hat y) =  \log \sum_{k=1}^q\exp(o_k) - \sum_{j=1}^qy_jo_j \tag{8}
$$
考虑上式对$o_j$的导数：
$$
\partial_{o_j} l(y,\hat y) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)}-y_j= softmax(o)_j - y_j
$$
