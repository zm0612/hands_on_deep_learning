> 参考资料：https://zh-v2.d2l.ai/chapter_linear-networks/index.html

# 线性神经网络总结

## 1.线性回归

这一小结的内容比较简单，总结起来说就是一个线性拟合的过程。

需要关注的点有时候样本数据过大时，可以随机选取小批量样本进行随机梯度下降，从而拟合模型参数。避免了全数据的传递和求导等耗时操作。

另一个很重要的点是，对于数据尽量减少使用for循环的操作，能使用矩阵运算就最好选择矩阵运算，速度会得到巨大的提高。

**正态分布与平方损失**

线性回归最终转化为最小二乘求解的原理需要关注一下，这里需要应用条件概率来解释。

对于线性回归问题，我们已知模型的结构，但是不知道模型的参数，另外还知道样本的输入，以及样本输入对应的真实输出。

那么如果用概率来建模这个问题，那么就是在已知输入$x$的情况下，要使得测量值$w^Tx+b$尽量的接近真实值$y$，那么此时的置信度将达到最大，此时对应的$w,b$就是我们要求的最优估计。

因此采用数学公式表示如下：
$$
P(y|x) = \frac{1}{\sqrt{2 \pi \sigma^2}}\exp{(-\frac{1}{2\sigma^2}(y-w^Tx-b)^2)} \tag{1}
$$
上式表示，在x的条件下得到y的概率。更通俗的解释就是，当$x$取某一个值的时候，$w^T+b$会得到一个测量值，也就是当前条件下的y的期望值，由于期望值和实际的真实值之间有误差，那么就会导致(1)式不会取得最大值，但是如果通过优化$w,b$就等使得$y-w^T-b$接近0，那么此时(1)式子将会取得最大概率。

那么当我们有很多观测时，那么要做的就是使得整体的条件概率达到最大。写出最大似然估计如下：
$$
P(y|X) = \prod_{i=1}^n p(y^{i}|x^{i}) \tag{2}
$$
通过取负log最终得到下式：
$$
-logP(y|X) = \sum_{i=1}^n \frac{1}{2}\log(2\pi\sigma^2) + \frac{1}{2\sigma^2}(y^{(i)}-w^Tx^{(i)}-b)^2 \tag{3}
$$
现在变成了最小化(3)式了，那么也就是所有误差取得最小的时候得到最大似然估计。

